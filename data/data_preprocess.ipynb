{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from gensim.summarization.textcleaner import clean_text_by_word\n",
    "from gensim import downloader\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(obj, pkl_name):\n",
    "    with open(pkl_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_pkl(pkl_name):\n",
    "    with open(pkl_name, 'rb') as f:\n",
    "        return pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../KGE/transe_wikidata5m.pkl\", \"rb\") as fin:\n",
    "    model = pickle.load(fin)\n",
    "entity2id = model.graph.entity2id\n",
    "\n",
    "entity_embeddings = model.solver.entity_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alias(alias_file):\n",
    "    alias2object = {}\n",
    "    ambiguous = set()\n",
    "    with open(alias_file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            tokens = line.strip().split(\"\\t\")\n",
    "            object = tokens[0]\n",
    "            for alias in tokens[1:]:\n",
    "                if alias in alias2object and alias2object[alias] != object:\n",
    "                    ambiguous.add(alias)\n",
    "                alias2object[alias] = object\n",
    "        for alias in ambiguous:\n",
    "            alias2object.pop(alias)\n",
    "    return alias2object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias2entity = load_alias('/media/yuting/TOSHIBA EXT/KGE/entity.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_entity = {}\n",
    "for entity in alias2entity:\n",
    "    try:\n",
    "        wiki_entity[entity] = entity_embeddings[entity2id[alias2entity[entity]]]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(wiki_entity,\"wiki_entity_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with one title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title10w = pd.read_csv('../title_data/title10w.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title10w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2title = dict(zip(list(title10w.twe_index),list(title10w.twe_title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dict1 index -> title words list dict2 index -> title entity\n",
    "index2words = {}\n",
    "index2entity = {}\n",
    "for index in index2title.keys():\n",
    "    \n",
    "    # generate word list in title\n",
    "    l_words = list(clean_text_by_word(index2title[index]).keys())\n",
    "    if 'title' in l_words:\n",
    "        l_words.remove('title')\n",
    "    index2words[index] = l_words\n",
    "    \n",
    "    # map the entity in word list\n",
    "    entity_list = []\n",
    "    for i in l_words:\n",
    "        try:\n",
    "            entity = entity2id[alias2entity[str(i)]] # make sure it's a str\n",
    "            entity_list.append(entity)\n",
    "        except:\n",
    "            continue\n",
    "    index2entity[index] = entity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "chunkSize = 200000\n",
    "twe_text = pd.DataFrame()\n",
    "chunk_df = pd.read_csv('/media/yuting/TOSHIBA EXT/retweet/retweetstext.csv', \\\n",
    "                       header=None, \\\n",
    "                       names=['id_retweet','text','date','user_re','user_orig','id_twe'],\\\n",
    "                       iterator=True, sep='#1#8#3#', \\\n",
    "                       chunksize=chunkSize)\n",
    "for chunk in chunk_df:\n",
    "    twe_text = twe_text.append(chunk)\n",
    "    break\n",
    "\n",
    "twe_text.replace(np.nan, 0, inplace=True)\n",
    "twe_text.replace(np.inf, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user statistics\n",
    "user_re_list = list(dict(twe_text['user_re'].value_counts()).keys())\n",
    "print(len(user_re_list))\n",
    "user_orig_list = list(dict(twe_text['user_orig'].value_counts()).keys())\n",
    "print(len(user_orig_list))\n",
    "# common_user = [x for x in user_re_list if x in user_orig_list] #len(common_user) = 1296 #100w\n",
    "# user_orig_not_re = [x for x in user_orig_list if x not in user_re_lsit] # len(user_orig_not_re) = 374598 # 100w\n",
    "print(len(common_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict user_index (index of all the tweets of chosed users)\n",
    "users_re_index = dict()\n",
    "for user in user_re_list:\n",
    "    l_index = twe_text[twe_text['user_re'] == user].index.tolist()  # index of twe for users\n",
    "    users_re_index[user] = l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict user_index (index of all the tweets connect to news links)\n",
    "users_re_index1 = dict()\n",
    "for user in user_re_list:\n",
    "    l_index1 = [x for x in users_re_index[user] if x in list(index2title.keys())]\n",
    "    users_re_index1[user] = l_index1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapping user to words and entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the retweet text with news links\n",
    "twe_text_selected = twe_text.loc[list(title10w.twe_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twe_text_selected['title_words'] = twe_text_selected.index.map(lambda x: index2words[x])\n",
    "twe_text_selected['title_entity'] = twe_text_selected.index.map(lambda x: index2entity[x])\n",
    "twe_text_selected['twe_index'] = list(title10w['twe_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twe_text_selected[['id_retweet','user_re','user_orig','title_words','title_entity','twe_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict key:user_re, value:list of (user_orig, title_words, entities)\n",
    "users_re_info = dict()\n",
    "for user in user_re_list:\n",
    "    user_orig_list = twe_text_selected[twe_text_selected['user_re'] == user].user_orig.tolist()\n",
    "    words_list = twe_text_selected[twe_text_selected['user_re'] == user].title_words.tolist()\n",
    "    entities_list = twe_text_selected[twe_text_selected['user_re'] == user].title_entity.tolist()\n",
    "    users_re_info[user] = list(zip(user_orig_list,words_list,entities_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# node embedding & word embeddings & entitiy embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding\n",
    "model_word = downloader.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load node embedding\n",
    "nodes_embedding = load_pkl('../weighted graph/sample_network_embedding_20w_100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict key: user_re, value:list of tuple (user_orig_emb, list of title_words_emb, list of entity_emb) \n",
    "users_info_embedding = {}\n",
    "for key in users_re_info.keys():\n",
    "    if len(users_re_info[key]) != 0:\n",
    "        user_emb_node_title_entity = []\n",
    "        for i in range(len(users_re_info[key])):\n",
    "            orig_node = nodes_embedding[str(int(users_re_info[key][i][0]))]\n",
    "            word_embed_list = []\n",
    "            if len(users_re_info[key][i][1]) != 0:\n",
    "                for j in range(len(users_re_info[key][i][1])):\n",
    "                    try:\n",
    "                        word_embed = model_word.word_vec(users_re_info[key][i][1][j])\n",
    "                        word_embed_list.append(word_embed)\n",
    "                    except:continue\n",
    "            entity_emb_list = []\n",
    "            if len(users_re_info[key][i][2]) != 0:\n",
    "                for k in range(len(users_re_info[key][i][2])):\n",
    "                    try:\n",
    "                        entity_emb = entity_embeddings[users_re_info[key][i][2][k]]\n",
    "                        entity_emb_list.append(entity_emb)\n",
    "                    except:continue\n",
    "            user_emb_node_title_entity.append((orig_node,word_embed_list,entity_emb_list))\n",
    "    users_info_embedding[key] = user_emb_node_title_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(users_info_embedding, 'users_info_embedding_entity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entity statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity = pd.DataFrame(zip(list(index2entity.keys()),list(index2entity.values())), columns=['index','entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity['entities_len'] = df_entity.entities.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2entity_len = dict(zip(list(df_entity.index),list(df_entity.entities_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "max_freq = max(index2entity_len.values())\n",
    "print(max_freq)\n",
    "bins = np.arange(0, 24,1)\n",
    "plt.hist(index2entity_len.values(), bins, alpha=0.8, label='# history', color='blue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('ditribution of user history number')\n",
    "\n",
    "plt.xlabel('# history news')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapping users to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2title.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_words = list(clean_text_by_word(index2title[13]).keys())\n",
    "if 'title' in l_words:\n",
    "        l_words.remove('title')\n",
    "l_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_emb = []\n",
    "for i in range(len(l_words)):\n",
    "    try:\n",
    "        word_emb = model_word.word_vec(l_words[i])\n",
    "        l_emb.append(word_emb)\n",
    "    except:continue\n",
    "l_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2title_words = {}\n",
    "index2title_emb = {}\n",
    "for index in index2title:\n",
    "    l_words = list(clean_text_by_word(index2title[index]).keys())\n",
    "    if 'title' in l_words:\n",
    "        l_words.remove('title')\n",
    "    index2title_words[index] = l_words\n",
    "    \n",
    "    l_words_emb = []\n",
    "    for i in range(len(l_words)):\n",
    "        try:\n",
    "            word_emb = model_word.word_vec(l_words[i])\n",
    "            l_words_emb.append(word_emb)\n",
    "        except:continue\n",
    "    index2title_emb[index] = l_words_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose the retweet text with news links\n",
    "twe_text_selected = twe_text.loc[list(title10w.twe_index)]\n",
    "twe_text_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twe_text_selected['title_words'] = twe_text_selected.index.map(lambda x: index2words[x])\n",
    "twe_text_selected['twe_index'] = list(title10w['twe_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twe_text_selected = twe_text_selected[['id_retweet','user_re','user_orig','id_twe','title_words','twe_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2orig = dict(zip(list(twe_text_selected.twe_index),list(twe_text_selected.user_orig)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history number dict\n",
    "uid2titlesNum = dict(twe_text_selected['user_re'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "max_freq = max(uid2titlesNum.values())\n",
    "print(max_freq)\n",
    "bins = np.arange(0, 80,1)\n",
    "plt.hist(uid2titlesNum.values(), bins, alpha=0.8, label='# history', color='blue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('ditribution of user history number')\n",
    "\n",
    "plt.xlabel('# history news')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1\n",
    "uid_list = [k for k,v in uid2titlesNum.items() if v >= threshold]\n",
    "len(uid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = twe_text_selected[twe_text_selected.user_re.map(lambda x: x in uid_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.title_words.map(len)>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid2index = df.groupby('user_re').twe_index.apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordering through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in: dict key: user id, value: tuple(timestamp, user_orig, title)\n",
    "# out: dict key: user id, value: tutple (user_orig, title) (arranged by time)\n",
    "for user in dictHistory:\n",
    "    l_new = dictHistory[user].sort(lambda x: x(0))\n",
    "    dictHistory[user] = l_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.dictHistory(dictHistory, orient='index',columns=['history'])\n",
    "df = df.reset_index().rename(columns = {'index':'id_retweet'})\n",
    "df['user_orig'] = df.history.map(lambda x: x[0])\n",
    "df['title_words'] = df.history.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# negtive sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neg_sample = 4 # train setup\n",
    "num_neg_sample = 10 # test setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSampling(uid2titles, user, num_neg):\n",
    "    \n",
    "    samplingDict = {}\n",
    "    samplingDict = copy.deepcopy(uid2titles)\n",
    "    del samplingDict[user]  # remove the target user\n",
    "    negPool = sum(list(samplingDict.values()),[])\n",
    "    negSample = random.sample(negPool,num_neg)\n",
    "    \n",
    "    return negSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid2neg = {}\n",
    "for user in uid2index.keys():\n",
    "    negSample = negSampling(uid2index, user, num_neg_sample)\n",
    "    uid2neg[user] = negSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fill dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid2pos = uid2index\n",
    "l1 = list(uid2neg.keys())\n",
    "l2 = list(uid2pos.keys())\n",
    "assert l1 == l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.DataFrame([(k,) + (ele,) + (1,) for k, v in uid2pos.items() for ele in v], columns=['uid', 'news','labels'])\n",
    "df_neg = pd.DataFrame([(k,) + (ele,) + (0,) for k, v in uid2neg.items() for ele in v], columns=['uid', 'news','labels'])\n",
    "\n",
    "df = pd.concat([df_pos,df_neg], axis=0)\n",
    "\n",
    "history_num = 25\n",
    "uid2news_fixed_len = {}\n",
    "for user in uid2index.keys():\n",
    "    indices = np.random.choice(list(range(0, len(uid2index[user]))), size=history_num, replace=True)\n",
    "    chose_news = np.array(uid2index[user])[indices]\n",
    "    uid2news_fixed_len[user] = chose_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['history_titles'] = df['uid'].map(lambda x: uid2news_fixed_len[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['orig_uid'] = df.news.map(lambda x: index2orig[x])\n",
    "df['history_orig_uid'] = df.history_titles.map(lambda x: [index2orig[y] for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/yuting/TOSHIBA EXT/retweet/diffusion/twitter/embeddings/infector_target3.txt'\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_number = re.compile('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?')\n",
    "node2emb = {}\n",
    "emb = ''\n",
    "for i in range(len(lines)):\n",
    "    \n",
    "    if not lines[i].endswith(']\\n'):\n",
    "        emb = emb + lines[i]\n",
    "        continue\n",
    "    else:\n",
    "        emb = emb + lines[i]\n",
    "        l_temp = re.findall(match_number,emb)\n",
    "        assert len(l_temp)==51\n",
    "        number = [float(x) for x in l_temp[1:]]\n",
    "        node2emb[l_temp[0]] =  number\n",
    "        emb = ''\n",
    "#     l_temp = re.findall(r\"-?\\d+\\.?\\d*e??\\d*?\",emb)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapNode(x,node2embed):\n",
    "    user = str(int(x))\n",
    "    try:\n",
    "        emb = node2embed[user]\n",
    "    except:\n",
    "        emb = np.nan\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapHistoryNode(x,node2embed):\n",
    "    user = str(int(x))\n",
    "    try:\n",
    "        emb = node2embed[user]\n",
    "    except:\n",
    "        emb = [0] * 50\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['orig_node'] = df.orig_uid.apply(mapNode, args=(node2emb,))\n",
    "df.dropna(inplace=True)\n",
    "df['history_nodes'] = df.history_orig_uid.apply(lambda x: [mapHistoryNode(i,node2emb) for i in x])\n",
    "df['news_emb'] = df.news.map(lambda x: index2title_emb[x])\n",
    "df['history_news'] = df.history_titles.map(lambda x: [index2title_emb[x[i]] for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_title_len = 10\n",
    "embed_dim =100\n",
    "nb_history = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend title length\n",
    "def titleLenExtend(title, max_title_len, embed_dim):\n",
    "    if len(title) >= max_title_len:\n",
    "        title = title[:max_title_len]\n",
    "    else:\n",
    "        m = int(max_title_len - len(title))\n",
    "        addLen = [np.zeros(([embed_dim]),dtype=np.float32) for i in range(m)]\n",
    "        title.extend(addLen)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_test = [titleLenExtend(df.history_news.iloc[0][i],10,100) for i in range(nb_history)]\n",
    "df.news_emb = df.news_emb.apply(titleLenExtend,args=(max_title_len,embed_dim))\n",
    "df.history_news = df.history_news.apply(lambda x:[titleLenExtend(x[i],max_title_len,embed_dim) for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['shape_hsitory_news'] = df.history_news.map(np.shape)\n",
    "df['shape_news'] = df.news_emb.map(np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(df,'formed_data_history20_neg10_influence_emb_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(df,'formed_data_history20_neg10_influence_emb_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (shap)",
   "language": "python",
   "name": "pycharm-81b7abe8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}